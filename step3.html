<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Absolute Dissociation Free Energy calculations on HPCs: vDSSB tutorial for GROMACS users | vdssb_gromacs</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Absolute Dissociation Free Energy calculations on HPCs: vDSSB tutorial for GROMACS users" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="test pages" />
<meta property="og:description" content="test pages" />
<link rel="canonical" href="https://procacci.github.io/vdssb_gromacs/" />
<meta property="og:url" content="https://procacci.github.io/vdssb_gromacs/" />
<meta property="og:site_name" content="vdssb_gromacs" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Absolute Dissociation Free Energy calculations on HPCs: vDSSB tutorial for GROMACS users" />
<script type="application/ld+json">
{"headline":"Absolute Dissociation Free Energy calculations on HPCs: vDSSB tutorial for GROMACS users","description":"test pages",
"url":"https://procacci.github.io/vdssb_gromacs/","@type":"WebSite","name":"vdssb_gromacs","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/vdssb_gromacs/assets/css/style.css?v=16c48c577b78999daacffb7c89c7e831c9ca80b9">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/vdssb_gromacs/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <!-- <h1><a href="https://procacci.github.io/vdssb_gromacs/">vdssb_gromacs</a></h1> -->
      

<h1 id="step-3-hrem-simulations-hpc">Step 3:  HREM simulations (HPC) </h1>
Copy the two directories <code>
  3clp_HREM_OUT </code> and <code> LIG_only_ligand_HREM_OUT</code></b> from
your workstation to the remote HPC working directory. E.g. from the working directory
where you launched the HPC_Drug command in <a href="step2.html"> Step 2</a>,  just issue 

<pre>
  scp -r 3clp_HREM_OUT user@hpcaddress:USERHOME
  scp -r LIG_only_ligand_HREM_OUT  hpc:USERHOME
</pre>

where <code> user@hpcaddress </code> is the username and
address of your HPC account and <code> USERHOME </code> is your home
directory at the HPC front-end.<br>
<br>
<b> Now login to your HPC account.</b> <br>
<br>

<p style="text-align:justify"> On most HPC platforms, disk quotas on non volatile storage are
limited. In this case it may be necessary to copy the two HREM
directories to the HPC scratch area prior to execution, as the present
computational Step 3 will generate several tens of GB of data on
the HPC:<br>
  <pre>
cp USERHOME/3clp_HREM USER_SCRATCH/bound
cp USERHOME/LIG_only_ligand_HREM USER_SCRATCH/unbound
</pre>
where <code>USER_SCRATCH</code> is the user scratch area on the HPC.

In each of the two folders <code>bound</code> and <code>unbound </code>, you will find a script file named <code>
MAKETPRFILES.sh</code>, one for the <a href="bound.html"> bound
state</a> run (ligand annihilation in thye complex) and one for the
<a href=unbound.html> unbound state</a> run (ligand growth in the
solvent).  This scripts serve to generate all GROMACS <code>
tpr </code> files that are necessary to run the HREM simulations on
the HPC using the HPC_Drug-generated mdp and top and
gro <a href="https://zenodo.org/record/5139374"> files</a>
  in <a href="step2.html"> Step 2</a>.</p><br>

<p style="text-align:justify">Once you execute interactively the <code> MAKETPRFILES.sh</code>
scripts on the HPC_Drug-generated HREM directories, you are ready to
submit you parallel jobs for vDSSB enhanced sampling on the HPC.
To this end, in <a href="step2.html"> Step 2</a>, HPC_Drug also
generated, in each of the two directories
<code>
  3clp_HREM_OUT </code> and <code>
LIG_only_ligand_HREM_OUT</code></b>, two tentative batch files for HPC
submission, based on the syntax of the
<a href="https://slurm.schedmd.com/sbatch.html"> SLURM</a> workload
manager.  These two SLURM submission files,
for <a href="bound_slurm.html"> bound-state</a>
and <a href="unbound_slurm.html"> unbound-state</a>, afford the
enhanced sampling of the vDSSB end-state for the complex
PF-07321332-3CL<sup>pro</sup> on the
heterogeneous <a href="https://www.hpc.cineca.it/hardware/marconi100">
Marconi100</a> HPC platform (CINECA), equipped with 4 Nvidia VOLTA
  GPU per node.</p> <br> On Marconi100, the jobs are submitted independently with
the commands:
<pre>
  cd USER_SCRATCH/bound
  ./MAKETPRFILES.sh 
  sbatch  HREM_input.slr
  
  cd USER_SCRATCH/unbound
  ./MAKETPRFILES.sh 
  sbatch  HREM_input.slr   
</pre>
The <a href="bound_slurm.html"> bound-state</a>
and <a href="unbound_slurm.html"> unbound-state</a> jobs requests 36
nodes (144 Nvidia VOLTA GPUs) and 8 nodes (32 Nvidia VOLTA GPU),
respectively.  The job relative to the bound state produces about 142
ns on the target state in 24 wall clock hours, running six replicates
of 24-replica exchange simulation involving a <i> hot-zone </i>  including the
ligand and nearby residues. The job relative to the unbound state requests
8 nodes (32 Nvidia VOLTA GPUs) and produces 32
ns on the target state in 5/6 wall clock hours, running four replicates
of 8-replica exchange simulation with torsional trempering of the full
  ligand.</p><br>
<br>
<br>
<p style="text-align:justify"><b> N.B.(1):</b> On the HPC platforms, <code> GROMACS </code> is
usually made available by issuing a specific <code> module
load </code> directive prior to submission or directly into the batch
submission scripts (see e.g. the bound-state
SLURM <a href="bound_slurm.html"> script</a>).  The HREM execution
requires GROMACS to be patched with PLUMED. If the GROMACS-PLUMED
module is not available on the HPC, then the user must compile and
patch GROMACS with PLUMED on the HPC before submission, generating his/her own
<code> gmx_mpi </code> executable, and change the
SLURM script specifying the full path of the PLUMED-patched <code> gmx_mpi </code>
command.   Compiling and patching GROMACS with PLUMED is described <a href="https://www.plumed.org/doc-v2.6/user-doc/html/_installation.html">
  here</a> (section <b> Patching your MD code </b>). </p><br><br>


<p style="text-align:justify"><b> N.B.(2):</b> The provided SLURM files (for <a href="bound_slurm.html"> bound-state</a> run  and <a href="unbound_slurm.html"> 
  unbound-state</a> run) <b> must </b> be hacked and adapted to the specific HPC
platform job scheduling/accounting system by the end-user. In the <a href="https://zenodo.org/record/5139374"> Zenodo</a>
repository, a <a href="https://www.jlab.org/hpc/PBS/qsub.html"> PBS</a> script for batch submission is also provided.</p><br>  

  
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" 
            crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>
